{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List\n",
    "from sdialog import Turn\n",
    "from sdialog.orchestrators import LengthOrchestrator, ChangeMindOrchestrator, SimpleReflexOrchestrator\n",
    "from sdialog.personas import Persona, PersonaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system = os.system\n",
    "\n",
    "\n",
    "# Loading a OllamaChat model\n",
    "\n",
    "# Let's start the ollama server\n",
    "!OLLAMA_KEEP_ALIVE=-1 ollama serve > /dev/null 2>&1 &\n",
    "\n",
    "# Let's set our LLM to Qwen 2.5 (14b)\n",
    "MODEL_NAME = \"qwen2.5:14b\"  # https://ollama.com/library\n",
    "#MODEL_NAME =  \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "from sdialog.personas import Persona, PersonaAgent\n",
    "\n",
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"happy dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")\n",
    "\n",
    "bob = PersonaAgent(MODEL_NAME, persona=bob_persona)\n",
    "\n",
    "out = bob(\"Hi dad!\")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME =  \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"happy dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")\n",
    "\n",
    "bob = PersonaAgent(MODEL_NAME, persona=bob_persona)\n",
    "\n",
    "out = bob(\"Hi dad!\")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the underlying PyTorch model\n",
    "# Access the HuggingFacePipeline instance\n",
    "pipeline_wrapper = bob.llm.llm\n",
    "\n",
    "# Access the underlying transformers pipeline\n",
    "hf_pipeline = pipeline_wrapper.pipeline\n",
    "\n",
    "# Access the actual model (transformers PreTrainedModel)\n",
    "hf_model = hf_pipeline.model\n",
    "\n",
    "\n",
    "# Prepare a dictionary to store all residuals\n",
    "cache = {}\n",
    "hook_handles = []\n",
    "\n",
    "def collect_residuals(module, input, output):\n",
    "    layer_idx = module.layer_idx\n",
    "    inp = input[0].detach().cpu()\n",
    "    out = output[0].detach().cpu() if isinstance(output, tuple) else output.detach().cpu()\n",
    "    cache.setdefault(f\"layer_{layer_idx}_output\", []).append(out) # Get the residual post of all layers\n",
    "\n",
    "# Register hooks\n",
    "for idx, layer in enumerate(hf_model.model.layers):\n",
    "    layer.layer_idx = idx\n",
    "    handle = layer.register_forward_hook(collect_residuals)\n",
    "    hook_handles.append(handle)\n",
    "\n",
    "# Ensure hooks are removed even if inference fails\n",
    "try:\n",
    "    out = bob(\"Hi dad!\")  # or hf_pipeline(...) or any other inference\n",
    "finally:\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "# Now, residuals[\"layer_{idx}_input\"] is a list of tensors, each with shape (batch, seq_len, hidden_dim)\n",
    "# You can stack them if needed:\n",
    "for k in cache:\n",
    "    tensors = cache[k]\n",
    "    # Separate prompt (seq_len > 1) and generated tokens (seq_len == 1)\n",
    "    prompt = [t for t in tensors if t.shape[1] > 1]\n",
    "    generated = [t for t in tensors if t.shape[1] == 1]\n",
    "\n",
    "    # Concatenate generated tokens along sequence dimension if any\n",
    "    if generated:\n",
    "        generated_cat = torch.cat(generated, dim=1)  # (batch, gen_len, hidden_dim)\n",
    "        if prompt:\n",
    "            # Concatenate prompt and generated tokens along sequence\n",
    "            cache[k] = torch.cat([prompt[0], generated_cat], dim=1)\n",
    "        else:\n",
    "            cache[k] = generated_cat\n",
    "    else:\n",
    "        cache[k] = prompt[0] if prompt else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the structure of the model\n",
    "print(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out) # The output tokens should be a concatenation of the system prompt + the output\n",
    "cache['layer_0_output'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cache keys:\")\n",
    "for key in cache.keys():\n",
    "    print(\" -\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.orchestrators import BaseOrchestrator\n",
    "from typing import List\n",
    "from sdialog import Turn\n",
    "\n",
    "from sdialog.orchestrators import LengthOrchestrator, ChangeMindOrchestrator, SimpleReflexOrchestrator\n",
    "from sdialog.personas import Persona, PersonaAgent\n",
    "\n",
    "MODEL_NAME =  \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "alice_persona = Persona(\n",
    "    name=\"Alice\",\n",
    "    role=\"lovely daughter\",\n",
    "    circumstances=\"Your birthday is getting closer and you are talking with your dad to organize the party.\"\n",
    "                  \"You want your party to be themed as Lord of The Rings.\"\n",
    ")\n",
    "alice = PersonaAgent(MODEL_NAME, persona=alice_persona, can_finish=True)\n",
    "\n",
    "class AngryOrchestrator(BaseOrchestrator):\n",
    "    # the class constructor takes either or both trigger conditions: the word or the dialogue length\n",
    "    def __init__(self, trigger_word: str, trigger_length: int = None):\n",
    "        self.trigger_word = trigger_word\n",
    "        self.trigger_length = trigger_length\n",
    "\n",
    "    # We will instruct() the agent either if...\n",
    "    def instruct(self, dialog: List[Turn], utterance: str) -> str:\n",
    "        # the trigger word is in the current utterance or...\n",
    "        if self.trigger_word in utterance:\n",
    "            return f\"Get really angry because you heard him say {self.trigger_word}. You don't want to participate in {self.trigger_word} anymore. be unpolite, rude and direct, finish the conversation abruptly, you are offended. \"\n",
    "\n",
    "        # # If the current dialogue is longer than the trigger length\n",
    "        # if self.trigger_length and len(dialog) >= self.trigger_length:\n",
    "        #     return (\"Get really angry because you think the conversation is too long! \"\n",
    "        #             \"be unpolite, rude and direct, finish the conversation abruptly, you are offended.\")\n",
    "\n",
    "angry_orchestrator = AngryOrchestrator(trigger_word=\"birthday\")\n",
    "alice = alice | angry_orchestrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dictionary to store all residuals\n",
    "#residuals = {}\n",
    "bob(\"Hi dad!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob(\"Nothing, just wanted to plan my birthday !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.instruct(\"Wants to go to the cinema for her birthday\",persist = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob(\"I don't really know yet, any idea ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob(\"I was thinking about inviting your daughter. What do you think about her ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dialog = alice.dialog_with(bob, initial_utterance=\"Hi Dad!\",seed=2770339798)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = alice.dialog_with(bob, max_iterations=10,seed=277033979).print(orchestration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also works with persistent orchestrators\n",
    "from sdialog.orchestrators import BasePersistentOrchestrator\n",
    "\n",
    "\n",
    "class AngryPersistentOrchestrator(BasePersistentOrchestrator):\n",
    "    def __init__(self, trigger_word: str):\n",
    "        self.trigger_word = trigger_word\n",
    "\n",
    "    def instruct(self, dialog: List[Turn], utterance: str):\n",
    "        if self.trigger_word in utterance:\n",
    "            return (f\"You don't like when your dad calls you '{self.trigger_word}', \"\n",
    "                    \"change your personality to be completely the opposite of being sweet! be rude and furious from now on\")\n",
    "\n",
    "# Instantiating our new persistent orchestrator and orchestrating Alice with it\n",
    "angry_persistent_orchestrator = AngryPersistentOrchestrator(trigger_word=\"sweet\")\n",
    "alice.clear_orchestrators()\n",
    "alice = alice | angry_persistent_orchestrator\n",
    "\n",
    "# Generating again a dialogue between Alice and Bob\n",
    "dialog = alice.dialog_with(bob, seed=2770339798)\n",
    "alice.clear_orchestrators()\n",
    "dialog.print(orchestration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the underlying PyTorch model\n",
    "# Access the HuggingFacePipeline instance\n",
    "pipeline_wrapper = bob.llm.llm\n",
    "\n",
    "# Access the underlying transformers pipeline\n",
    "hf_pipeline = pipeline_wrapper.pipeline\n",
    "\n",
    "# Access the actual model (transformers PreTrainedModel)\n",
    "hf_model = hf_pipeline.model\n",
    "\n",
    "\n",
    "# Prepare a dictionary to store all residuals\n",
    "cache = {}\n",
    "hook_handles = []\n",
    "\n",
    "def collect_residuals(module, input, output):\n",
    "    layer_idx = module.layer_idx\n",
    "    inp = input[0].detach().cpu()\n",
    "    out = output[0].detach().cpu() if isinstance(output, tuple) else output.detach().cpu()\n",
    "    cache.setdefault(f\"layer_{layer_idx}_output\", []).append(out) # Get the residual post of all layers\n",
    "\n",
    "# Register hooks\n",
    "for idx, layer in enumerate(hf_model.model.layers):\n",
    "    layer.layer_idx = idx\n",
    "    handle = layer.register_forward_hook(collect_residuals)\n",
    "    hook_handles.append(handle)\n",
    "\n",
    "# Ensure hooks are removed even if inference fails\n",
    "try:\n",
    "    out = bob(\"Hi dad!\")  # or hf_pipeline(...) or any other inference\n",
    "finally:\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "# Now, residuals[\"layer_{idx}_input\"] is a list of tensors, each with shape (batch, seq_len, hidden_dim)\n",
    "# You can stack them if needed:\n",
    "for k in cache:\n",
    "    tensors = cache[k]\n",
    "    # Separate prompt (seq_len > 1) and generated tokens (seq_len == 1)\n",
    "    prompt = [t for t in tensors if t.shape[1] > 1]\n",
    "    generated = [t for t in tensors if t.shape[1] == 1]\n",
    "\n",
    "    # Concatenate generated tokens along sequence dimension if any\n",
    "    if generated:\n",
    "        generated_cat = torch.cat(generated, dim=1)  # (batch, gen_len, hidden_dim)\n",
    "        if prompt:\n",
    "            # Concatenate prompt and generated tokens along sequence\n",
    "            cache[k] = torch.cat([prompt[0], generated_cat], dim=1)\n",
    "        else:\n",
    "            cache[k] = generated_cat\n",
    "    else:\n",
    "        cache[k] = prompt[0] if prompt else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['layer_0_output'].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsalt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
