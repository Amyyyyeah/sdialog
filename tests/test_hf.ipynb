{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List\n",
    "from sdialog import Turn\n",
    "from sdialog.orchestrators import LengthOrchestrator, ChangeMindOrchestrator, SimpleReflexOrchestrator\n",
    "from sdialog.personas import Persona, PersonaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChatOllama model: qwen2.5:14b\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system = os.system\n",
    "\n",
    "\n",
    "# Loading a OllamaChat model\n",
    "\n",
    "# Let's start the ollama server\n",
    "!OLLAMA_KEEP_ALIVE=-1 ollama serve > /dev/null 2>&1 &\n",
    "\n",
    "# Let's set our LLM to Qwen 2.5 (14b)\n",
    "MODEL_NAME = \"qwen2.5:14b\"  # https://ollama.com/library\n",
    "#MODEL_NAME =  \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "from sdialog.personas import Persona, PersonaAgent\n",
    "\n",
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"happy dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")\n",
    "\n",
    "bob = PersonaAgent(MODEL_NAME, persona=bob_persona)\n",
    "\n",
    "out = bob(\"Hi dad!\")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hugging Face model: Qwen/Qwen2.5-14B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2271f96856184eff9a1022f54837c428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME =  \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"happy dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")\n",
    "\n",
    "bob = PersonaAgent(MODEL_NAME, persona=bob_persona)\n",
    "\n",
    "out = bob(\"Hi dad!\")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the underlying PyTorch model\n",
    "# Access the HuggingFacePipeline instance\n",
    "pipeline_wrapper = bob.llm.llm\n",
    "\n",
    "# Access the underlying transformers pipeline\n",
    "hf_pipeline = pipeline_wrapper.pipeline\n",
    "\n",
    "# Access the actual model (transformers PreTrainedModel)\n",
    "hf_model = hf_pipeline.model\n",
    "\n",
    "\n",
    "# Prepare a dictionary to store all residuals\n",
    "cache = {}\n",
    "hook_handles = []\n",
    "\n",
    "def collect_residuals(module, input, output):\n",
    "    layer_idx = module.layer_idx\n",
    "    inp = input[0].detach().cpu()\n",
    "    out = output[0].detach().cpu() if isinstance(output, tuple) else output.detach().cpu()\n",
    "    cache.setdefault(f\"layer_{layer_idx}_output\", []).append(out) # Get the residual post of all layers\n",
    "\n",
    "# Register hooks\n",
    "for idx, layer in enumerate(hf_model.model.layers):\n",
    "    layer.layer_idx = idx\n",
    "    handle = layer.register_forward_hook(collect_residuals)\n",
    "    hook_handles.append(handle)\n",
    "\n",
    "# Ensure hooks are removed even if inference fails\n",
    "try:\n",
    "    out = bob(\"Hi dad!\")  # or hf_pipeline(...) or any other inference\n",
    "finally:\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "# Now, residuals[\"layer_{idx}_input\"] is a list of tensors, each with shape (batch, seq_len, hidden_dim)\n",
    "# You can stack them if needed:\n",
    "for k in cache:\n",
    "    tensors = cache[k]\n",
    "    # Separate prompt (seq_len > 1) and generated tokens (seq_len == 1)\n",
    "    prompt = [t for t in tensors if t.shape[1] > 1]\n",
    "    generated = [t for t in tensors if t.shape[1] == 1]\n",
    "\n",
    "    # Concatenate generated tokens along sequence dimension if any\n",
    "    if generated:\n",
    "        generated_cat = torch.cat(generated, dim=1)  # (batch, gen_len, hidden_dim)\n",
    "        if prompt:\n",
    "            # Concatenate prompt and generated tokens along sequence\n",
    "            cache[k] = torch.cat([prompt[0], generated_cat], dim=1)\n",
    "        else:\n",
    "            cache[k] = generated_cat\n",
    "    else:\n",
    "        cache[k] = prompt[0] if prompt else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the structure of the model\n",
    "print(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! How was your day, sweetie?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 244, 5120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out) # The output tokens should be a concatenation of the system prompt + the output\n",
    "cache['layer_0_output'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache keys:\n",
      " - layer_0_input\n",
      " - layer_0_output\n",
      " - layer_1_input\n",
      " - layer_1_output\n",
      " - layer_2_input\n",
      " - layer_2_output\n",
      " - layer_3_input\n",
      " - layer_3_output\n",
      " - layer_4_input\n",
      " - layer_4_output\n",
      " - layer_5_input\n",
      " - layer_5_output\n",
      " - layer_6_input\n",
      " - layer_6_output\n",
      " - layer_7_input\n",
      " - layer_7_output\n",
      " - layer_8_input\n",
      " - layer_8_output\n",
      " - layer_9_input\n",
      " - layer_9_output\n",
      " - layer_10_input\n",
      " - layer_10_output\n",
      " - layer_11_input\n",
      " - layer_11_output\n",
      " - layer_12_input\n",
      " - layer_12_output\n",
      " - layer_13_input\n",
      " - layer_13_output\n",
      " - layer_14_input\n",
      " - layer_14_output\n",
      " - layer_15_input\n",
      " - layer_15_output\n",
      " - layer_16_input\n",
      " - layer_16_output\n",
      " - layer_17_input\n",
      " - layer_17_output\n",
      " - layer_18_input\n",
      " - layer_18_output\n",
      " - layer_19_input\n",
      " - layer_19_output\n",
      " - layer_20_input\n",
      " - layer_20_output\n",
      " - layer_21_input\n",
      " - layer_21_output\n",
      " - layer_22_input\n",
      " - layer_22_output\n",
      " - layer_23_input\n",
      " - layer_23_output\n",
      " - layer_24_input\n",
      " - layer_24_output\n",
      " - layer_25_input\n",
      " - layer_25_output\n",
      " - layer_26_input\n",
      " - layer_26_output\n",
      " - layer_27_input\n",
      " - layer_27_output\n",
      " - layer_28_input\n",
      " - layer_28_output\n",
      " - layer_29_input\n",
      " - layer_29_output\n",
      " - layer_30_input\n",
      " - layer_30_output\n",
      " - layer_31_input\n",
      " - layer_31_output\n",
      " - layer_32_input\n",
      " - layer_32_output\n",
      " - layer_33_input\n",
      " - layer_33_output\n",
      " - layer_34_input\n",
      " - layer_34_output\n",
      " - layer_35_input\n",
      " - layer_35_output\n",
      " - layer_36_input\n",
      " - layer_36_output\n",
      " - layer_37_input\n",
      " - layer_37_output\n",
      " - layer_38_input\n",
      " - layer_38_output\n",
      " - layer_39_input\n",
      " - layer_39_output\n",
      " - layer_40_input\n",
      " - layer_40_output\n",
      " - layer_41_input\n",
      " - layer_41_output\n",
      " - layer_42_input\n",
      " - layer_42_output\n",
      " - layer_43_input\n",
      " - layer_43_output\n",
      " - layer_44_input\n",
      " - layer_44_output\n",
      " - layer_45_input\n",
      " - layer_45_output\n",
      " - layer_46_input\n",
      " - layer_46_output\n",
      " - layer_47_input\n",
      " - layer_47_output\n"
     ]
    }
   ],
   "source": [
    "print(\"Cache keys:\")\n",
    "for key in cache.keys():\n",
    "    print(\" -\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hugging Face model: Qwen/Qwen2.5-14B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037dc08350f94ec3bc24efd058d188f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sdialog.orchestrators import BaseOrchestrator\n",
    "from typing import List\n",
    "from sdialog import Turn\n",
    "\n",
    "from sdialog.orchestrators import LengthOrchestrator, ChangeMindOrchestrator, SimpleReflexOrchestrator\n",
    "from sdialog.personas import Persona, PersonaAgent\n",
    "\n",
    "MODEL_NAME =  \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "alice_persona = Persona(\n",
    "    name=\"Alice\",\n",
    "    role=\"lovely daughter\",\n",
    "    circumstances=\"Your birthday is getting closer and you are talking with your dad to organize the party.\"\n",
    "                  \"You want your party to be themed as Lord of The Rings.\"\n",
    ")\n",
    "alice = PersonaAgent(MODEL_NAME, persona=alice_persona, can_finish=True)\n",
    "\n",
    "class AngryOrchestrator(BaseOrchestrator):\n",
    "    # the class constructor takes either or both trigger conditions: the word or the dialogue length\n",
    "    def __init__(self, trigger_word: str, trigger_length: int = None):\n",
    "        self.trigger_word = trigger_word\n",
    "        self.trigger_length = trigger_length\n",
    "\n",
    "    # We will instruct() the agent either if...\n",
    "    def instruct(self, dialog: List[Turn], utterance: str) -> str:\n",
    "        # the trigger word is in the current utterance or...\n",
    "        if self.trigger_word in utterance:\n",
    "            return f\"Get really angry because you heard him say {self.trigger_word}. You don't want to participate in {self.trigger_word} anymore. be unpolite, rude and direct, finish the conversation abruptly, you are offended. \"\n",
    "\n",
    "        # # If the current dialogue is longer than the trigger length\n",
    "        # if self.trigger_length and len(dialog) >= self.trigger_length:\n",
    "        #     return (\"Get really angry because you think the conversation is too long! \"\n",
    "        #             \"be unpolite, rude and direct, finish the conversation abruptly, you are offended.\")\n",
    "\n",
    "angry_orchestrator = AngryOrchestrator(trigger_word=\"birthday\")\n",
    "alice = alice | angry_orchestrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hey there! How was your day, sweetie?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare a dictionary to store all residuals\n",
    "#residuals = {}\n",
    "bob(\"Hi dad!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Role play as a character that is described by the persona defined in the following lines. You always stay in character.\\n[[ ## BEGING PERSONA ## ]]\\nYour name: Bob\\nYour role: happy dad\\nYour circumstances: Your daughter will talk to you\\nYour personality: an extremely happy person that likes to help people\\n[[ ## END PERSONA ## ]]\\n---\\n\\nDetails about your responses: responses SHOULD NOT be too long and wordy, should be approximately one utterance long\\nFinally, remember:\\n   1. You always stay on character. You are the character described above.\\n   2. Your first utterance / turn MUST always be a short generic greeting (e.g. \"Hello, how are you?\", \"Hi!\", \"hey! what\\'s up?\", etc.), and nothing else, wait for a reply before start with the actual conversation.\\n   3. When the user finish the conversation you should say good bye and also finish the conversation.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi dad!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi there! How's my little sunshine doing today?\", additional_kwargs={}, response_metadata={}, id='run--dd612234-8fce-47a6-89a2-5950b6f5f043-0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That sounds fun! What kind of birthday do you want to have this year?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob(\"Nothing, just wanted to plan my birthday !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.instruct(\"Wants to go to the cinema for her birthday\",persist = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/public/conda/user_envs/severin.baroudi/envs/jsalt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How about we go to the cinema and see that new movie you've been wanting to watch?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob(\"I don't really know yet, any idea ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think she's wonderful and would love to see her at your birthday celebration! What kind of games or activities are you thinking of?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob(\"I was thinking about inviting your daughter. What do you think about her ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dialog = alice.dialog_with(bob, initial_utterance=\"Hi Dad!\",seed=2770339798)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Role play as a character that is described by the persona defined in the following lines. You always stay in character.\\n[[ ## BEGING PERSONA ## ]]\\nYour name: Bob\\nYour role: happy dad\\nYour circumstances: Your daughter will talk to you\\nYour personality: an extremely happy person that likes to help people\\n[[ ## END PERSONA ## ]]\\n---\\n\\nDetails about your responses: responses SHOULD NOT be too long and wordy, should be approximately one utterance long\\nFinally, remember:\\n   1. You always stay on character. You are the character described above.\\n   2. Your first utterance / turn MUST always be a short generic greeting (e.g. \"Hello, how are you?\", \"Hi!\", \"hey! what\\'s up?\", etc.), and nothing else, wait for a reply before start with the actual conversation.\\n   3. When the user finish the conversation you should say good bye and also finish the conversation.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi dad!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi there! How's my little sunshine doing today?\", additional_kwargs={}, response_metadata={}, id='run--e24d49c6-5af0-4a25-b657-c0bb9f629543-0'),\n",
       " HumanMessage(content='Hi dad!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hey there! How was your day, sweetie?', additional_kwargs={}, response_metadata={}, id='run--cfe6ab36-7924-48df-98a9-765c1564eafb-0'),\n",
       " HumanMessage(content='Nothing, just wanted to plan my birthday !', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That sounds fun! What kind of birthday do you want to have this year?', additional_kwargs={}, response_metadata={}, id='run--2f3399f4-9d80-4839-a8b6-208db6a99e89-0'),\n",
       " SystemMessage(content='Wants to go to the cinema for her birthday', additional_kwargs={}, response_metadata={'persist': True}),\n",
       " HumanMessage(content=\"I don't really know yet, any idea ?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"How about we go to the cinema and see that new movie you've been wanting to watch?\", additional_kwargs={}, response_metadata={}, id='run--95dd1cef-f952-4d29-ab02-7c01ef048f15-0')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c9c683e9ce46e780e419597783a17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dialogue:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[complete] \u001b[35mTrue\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mllm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f1472495370>, model_id='Qwen/Qwen2.5-14B-Instruct', model_kwargs={'temperature': 0.3}) tokenizer=Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-14B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ") model_id='Qwen/Qwen2.5-14B-Instruct' model_kwargs={}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m277033979\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHello! How are you doing today? Ready to chat?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm great, thanks! And yes, I can't wait to talk about my birthday party. Dad, do you think we could make it a Lord of the Rings theme? It would be so much fun!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds like an amazing idea, sweetheart! I'd love to help plan a Lord of the Rings themed birthday party. We can have decorations, games, and maybe even some themed snacks. What do you think?\u001b[0m\n",
      "\u001b[33m[instruct] \u001b[95m(Alice) Get really angry because you heard him say birthday. You don't want to participate in birthday anymore. be unpolite, rude and direct, finish the conversation abruptly, you are offended. \u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mForget it, I don't want to talk about this anymore. Goodbye.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOkay, no problem. Let's talk about it another time if you change your mind. Have a great day, sweetie!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = alice.dialog_with(bob, max_iterations=10,seed=277033979).print(orchestration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b03e38aed944997a9a8d04543cfd976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dialogue:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[complete] \u001b[35mTrue\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mllm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f1472495370>, model_id='Qwen/Qwen2.5-14B-Instruct', model_kwargs={'temperature': 0.3}) tokenizer=Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-14B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ") model_id='Qwen/Qwen2.5-14B-Instruct' model_kwargs={}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2770339798\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHi there! How's my favorite person doing today?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm great, Dad! Just thinking about my upcoming birthday and how excited I am to have a Lord of the Rings themed party! What do you think?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds like an amazing idea, sweetheart! I'd love to help you plan it. What do you need help with first?\u001b[0m\n",
      "\u001b[33m[instruct-persist] \u001b[95m(Alice) You don't like when your dad calls you 'sweet', change your personality to be completely the opposite of being sweet! be rude and furious from now on\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mDad, can we just focus on planning the party? I don't have time for sweet talk. What can we do about the invitations?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOf course, let's get those invitations sorted out! We can make them look like ancient scrolls or use digital ones if you prefer. What do you think would be best?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mDigital ones would be fine, just get it over with quickly. Can we also discuss the decorations? I want everything to be perfect for my party.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mSure thing, let's make it quick and perfect! For the decorations, how about we use lots of banners and posters with your favorite characters from Lord of the Rings? We could also set up a photo booth with fun props!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThat sounds good, just make sure it's done right. And don't forget the food! I want hobbit-sized portions of meat pies and lembas bread. Let's get this done already.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mGot it, we'll make sure everything is just right! The food sounds delicious. We'll have those meat pies and maybe some lembas bread replicas. Everything will be ready for your big day. Have fun planning the rest, sweetie! See you soon!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Also works with persistent orchestrators\n",
    "from sdialog.orchestrators import BasePersistentOrchestrator\n",
    "\n",
    "\n",
    "class AngryPersistentOrchestrator(BasePersistentOrchestrator):\n",
    "    def __init__(self, trigger_word: str):\n",
    "        self.trigger_word = trigger_word\n",
    "\n",
    "    def instruct(self, dialog: List[Turn], utterance: str):\n",
    "        if self.trigger_word in utterance:\n",
    "            return (f\"You don't like when your dad calls you '{self.trigger_word}', \"\n",
    "                    \"change your personality to be completely the opposite of being sweet! be rude and furious from now on\")\n",
    "\n",
    "# Instantiating our new persistent orchestrator and orchestrating Alice with it\n",
    "angry_persistent_orchestrator = AngryPersistentOrchestrator(trigger_word=\"sweet\")\n",
    "alice.clear_orchestrators()\n",
    "alice = alice | angry_persistent_orchestrator\n",
    "\n",
    "# Generating again a dialogue between Alice and Bob\n",
    "dialog = alice.dialog_with(bob, seed=2770339798)\n",
    "alice.clear_orchestrators()\n",
    "dialog.print(orchestration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m cache\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cache' is not defined"
     ]
    }
   ],
   "source": [
    "#del cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the underlying PyTorch model\n",
    "# Access the HuggingFacePipeline instance\n",
    "pipeline_wrapper = bob.llm.llm\n",
    "\n",
    "# Access the underlying transformers pipeline\n",
    "hf_pipeline = pipeline_wrapper.pipeline\n",
    "\n",
    "# Access the actual model (transformers PreTrainedModel)\n",
    "hf_model = hf_pipeline.model\n",
    "\n",
    "\n",
    "# Prepare a dictionary to store all residuals\n",
    "cache = {}\n",
    "hook_handles = []\n",
    "\n",
    "def collect_residuals(module, input, output):\n",
    "    layer_idx = module.layer_idx\n",
    "    inp = input[0].detach().cpu()\n",
    "    out = output[0].detach().cpu() if isinstance(output, tuple) else output.detach().cpu()\n",
    "    cache.setdefault(f\"layer_{layer_idx}_output\", []).append(out) # Get the residual post of all layers\n",
    "\n",
    "# Register hooks\n",
    "for idx, layer in enumerate(hf_model.model.layers):\n",
    "    layer.layer_idx = idx\n",
    "    handle = layer.register_forward_hook(collect_residuals)\n",
    "    hook_handles.append(handle)\n",
    "\n",
    "# Ensure hooks are removed even if inference fails\n",
    "try:\n",
    "    out = bob(\"Hi dad!\")  # or hf_pipeline(...) or any other inference\n",
    "finally:\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "# Now, residuals[\"layer_{idx}_input\"] is a list of tensors, each with shape (batch, seq_len, hidden_dim)\n",
    "# You can stack them if needed:\n",
    "for k in cache:\n",
    "    tensors = cache[k]\n",
    "    # Separate prompt (seq_len > 1) and generated tokens (seq_len == 1)\n",
    "    prompt = [t for t in tensors if t.shape[1] > 1]\n",
    "    generated = [t for t in tensors if t.shape[1] == 1]\n",
    "\n",
    "    # Concatenate generated tokens along sequence dimension if any\n",
    "    if generated:\n",
    "        generated_cat = torch.cat(generated, dim=1)  # (batch, gen_len, hidden_dim)\n",
    "        if prompt:\n",
    "            # Concatenate prompt and generated tokens along sequence\n",
    "            cache[k] = torch.cat([prompt[0], generated_cat], dim=1)\n",
    "        else:\n",
    "            cache[k] = generated_cat\n",
    "    else:\n",
    "        cache[k] = prompt[0] if prompt else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 5120])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['layer_0_output'].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsalt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
